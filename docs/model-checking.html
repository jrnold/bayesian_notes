<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Updating: A Set of Bayesian Notes</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Updating: A Set of Bayesian Notes">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Updating: A Set of Bayesian Notes" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://jrnold.github.io/bayesian_notes" />
  
  
  <meta name="github-repo" content="jrnold/bayesian_notes" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Updating: A Set of Bayesian Notes" />
  <meta name="twitter:site" content="@jrnld" />
  
  

<meta name="author" content="Jeffrey B. Arnold">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="posterior-inference.html">


<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>

\[
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\mean}{mean}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Cor}{Cor}
\DeclareMathOperator{\Bias}{Bias}
\DeclareMathOperator{\MSE}{MSE}
\DeclareMathOperator{\RMSE}{RMSE}
\DeclareMathOperator{\sd}{sd}
\DeclareMathOperator{\se}{se}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\mat}[1]{\boldsymbol{#1}}
\newcommand{\vec}[1]{\boldsymbol{#1}}
\newcommand{\T}{'}

% This follows BDA
\newcommand{\dunif}{\mathrm{U}}
\newcommand{\dunif}{\mathrm{N}}
\newcommand{\dlnorm}{\mathrm{lognormal}}
\newcommand{\dmvnorm}{\mathrm{N}}
\newcommand{\dgamma}{\mathrm{Gamma}}
\newcommand{\dinvgamma}{\mathrm{Inv-Gamma}}
\newcommand{\dchisq}[1]{\chi^2_{#1}}
\newcommand{\dinvchisq}[1]{\mathrm{Inv-}\chi^2_{#1}}
\newcommand{\dexp}{\mathrm{Expon}}
\newcommand{\dlaplace}{\mathrm{Laplace}}
\newcommand{\dweibull}{\mathrm{Weibull}}
\newcommand{\dwishart}[1]{\mathrm{Wishart}_{#1}}
\newcommand{\dinvwishart}[1]{\mathrm{Inv-Wishart}_{#1}}
\newcommand{\dlkj}{\mathrm{LkjCorr}}
\newcommand{\dt}[1]{t_{#1}}
\newcommand{\dbeta}{\mathrm{Beta}}
\newcommand{\ddirichlet}{\mathrm{Dirichlet}}
\newcommand{\dlogistic}{\mathrm{Logistic}}
\newcommand{\dllogistic}{\mathrm{Log-logistic}}
\newcommand{\dpoisson}{\mathrm{Poisson}}
\newcommand{\dbinomial}{\mathrm{Bin}}
\newcommand{\dmultinom}{\mathrm{Multinom}}
\newcommand{\dnegbin}{\mathrm{Neg-bin}}
\newcommand{\dbetabinom}{\mathrm{Beta-bin}}
\newcommand{\dcauchy}{\mathrm{Cauchy}}
\newcommand{\dhalfcauchy}{\mathrm{Cauchy}^{+}}

\DeclareMathOperator{\logistic}{\Logistic}

\newcommand{\R}{\mathfrak{R}}
\newcommand{\N}{\mathfrak{N}}

\newcommand{\cia}{\perp\!\!\!\perp}
\DeclareMathOperator*{\plim}{plim}
\]

  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Bayesian Notes</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html"><i class="fa fa-check"></i><b>1</b> Introduction to Stan and Linear Regression</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#prerequites"><i class="fa fa-check"></i><b>1.1</b> Prerequites</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#the-statistical-model"><i class="fa fa-check"></i><b>1.2</b> The Statistical Model</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#sampling"><i class="fa fa-check"></i><b>1.2.1</b> Sampling</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#convergence-diagnostics-and-model-fit"><i class="fa fa-check"></i><b>1.2.2</b> Convergence Diagnostics and Model Fit</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction-to-stan-and-linear-regression.html"><a href="introduction-to-stan-and-linear-regression.html#maximum-a-posteriori-estimation"><i class="fa fa-check"></i><b>1.3</b> Maximum A Posteriori estimation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="posterior-inference.html"><a href="posterior-inference.html"><i class="fa fa-check"></i><b>2</b> Posterior Inference</a><ul>
<li class="chapter" data-level="2.1" data-path="posterior-inference.html"><a href="posterior-inference.html#prerequisites"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="2.1.1" data-path="posterior-inference.html"><a href="posterior-inference.html#introduction"><i class="fa fa-check"></i><b>2.1.1</b> Introduction</a></li>
<li class="chapter" data-level="2.1.2" data-path="posterior-inference.html"><a href="posterior-inference.html#functions-of-the-posterior-distribution"><i class="fa fa-check"></i><b>2.1.2</b> Functions of the Posterior Distribution</a></li>
<li class="chapter" data-level="2.1.3" data-path="posterior-inference.html"><a href="posterior-inference.html#marginal-effects"><i class="fa fa-check"></i><b>2.1.3</b> Marginal Effects</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="model-checking.html"><a href="model-checking.html"><i class="fa fa-check"></i><b>3</b> Model Checking</a><ul>
<li class="chapter" data-level="3.1" data-path="model-checking.html"><a href="model-checking.html#why-check-models"><i class="fa fa-check"></i><b>3.1</b> Why check models?</a></li>
<li class="chapter" data-level="3.2" data-path="model-checking.html"><a href="model-checking.html#posterior-predictive-checks"><i class="fa fa-check"></i><b>3.2</b> Posterior Predictive Checks</a><ul>
<li class="chapter" data-level="3.2.1" data-path="model-checking.html"><a href="model-checking.html#bayesian-p-values"><i class="fa fa-check"></i><b>3.2.1</b> Bayesian p-values</a></li>
<li class="chapter" data-level="3.2.2" data-path="model-checking.html"><a href="model-checking.html#test-quantities"><i class="fa fa-check"></i><b>3.2.2</b> Test quantities</a></li>
<li class="chapter" data-level="3.2.3" data-path="model-checking.html"><a href="model-checking.html#p-values-vs.u-values"><i class="fa fa-check"></i><b>3.2.3</b> p-values vs. u-values</a></li>
<li class="chapter" data-level="3.2.4" data-path="model-checking.html"><a href="model-checking.html#marginal-predictive-checks"><i class="fa fa-check"></i><b>3.2.4</b> Marginal predictive checks</a></li>
<li class="chapter" data-level="3.2.5" data-path="model-checking.html"><a href="model-checking.html#outliers"><i class="fa fa-check"></i><b>3.2.5</b> Outliers</a></li>
<li class="chapter" data-level="3.2.6" data-path="model-checking.html"><a href="model-checking.html#grapical-posterior-predictive-checks"><i class="fa fa-check"></i><b>3.2.6</b> Grapical Posterior Predictive Checks</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="model-checking.html"><a href="model-checking.html#sources"><i class="fa fa-check"></i><b>3.3</b> Sources</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Updating: A Set of Bayesian Notes</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-checking" class="section level1">
<h1><span class="header-section-number">3</span> Model Checking</h1>
<div id="why-check-models" class="section level2">
<h2><span class="header-section-number">3.1</span> Why check models?</h2>
<ul>
<li>In theory—Bayesian model should include all relevant substantive knowledge and subsume all possible theories.</li>
<li>In practice—It won’t. Need to check how the model fits data.</li>
</ul>
<p>The question is not whether a model is “true”; it isn’t <span class="citation">[@Box1976a]</span>. But is it good enough for the purposes of the analysis.</p>
<p>See <span class="citation">@GelmanMengStern1996a</span>, <span class="citation">@Gelman2007a</span>, <span class="citation">@Gelman2009a</span>, <span class="citation">@GelmanCarlinSternEtAl2013a [Ch. 6]</span>, <span class="citation">@GelmanShalizi2012a</span>, <span class="citation">@Kruschke2013b</span>, <span class="citation">@GelmanShalizi2012b</span>, <span class="citation">@Gelman2014</span> for more discussion of the motivation and use of posterior predictive checks.</p>
</div>
<div id="posterior-predictive-checks" class="section level2">
<h2><span class="header-section-number">3.2</span> Posterior Predictive Checks</h2>
<p>One way to evaluate the fit of a model is <strong>posterior predictive checks</strong></p>
<ul>
<li>Fit the model to the data to get the posterior distribution of the parameters: <span class="math inline">\(p(\theta | D)\)</span></li>
<li>Simulate data from the fitted model: <span class="math inline">\(p(\tilde{D} | \theta, D)\)</span></li>
<li>Compare the simulated data (or a statistic thereof) to the observed data and a statistic thereof. The comparison between data simulated from the model can be formal or visual.</li>
</ul>
<p>Within a Stan function, this is done in the <code>generated quantities</code> block using a <code>_rng</code> distribution functions:</p>
<pre class="stan"><code>generated quantities {
  vector[n] yrep;
  for (i in 1:n) {
    yrep[i] ~ 
  }
}</code></pre>
<p>The package <strong><a href="https://cran.r-project.org/package=bayesplot">bayesplot</a></strong> includes multiple functions for posterior predictive checks; see the help for <a href="https://www.rdocumentation.org/packages/bayesplot//topics/PPC-overview">PPC-overview</a> for a summary of these functions.</p>
<div id="bayesian-p-values" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Bayesian p-values</h3>
<p>A <strong>posterior predictive p-value</strong> is a the tail posterior probability for a statistic generated from the model compared to the statistic observed in the data. Let <span class="math inline">\(y = (y_1, \dots, y_n)\)</span> be the observed data. Suppose the model has been fit and there is a set of simulation <span class="math inline">\(\theta^(s)\)</span>, <span class="math inline">\(s = 1, \dots, n_sims\)</span>. In replicated dataset, <span class="math inline">\(y^{rep(s)\)</span>, has been generated from the predictive distribution of the data, <span class="math inline">\(p(y^{(rep)} | \theta = \theta^{(s)}\)</span>. Then the ensemble of simulated datasets, <span class="math inline">\((y^{rep(s)}, \dots, y^{rep(nsims)})\)</span>, is a sample from the posterior predictive distribution, <span class="math inline">\(p(y^{(rep)} | y)\)</span></p>
<p>The model can be tested by means of discrepancy statistics, which are some function of the data and parameters, <span class="math inline">\(T(y, \theta)\)</span>. If <span class="math inline">\(\theta\)</span> was known, then compare discrepancy by <span class="math inline">\(T(y^{(rep)}, \theta)\)</span>. The statistical significance is <span class="math inline">\(p = \Pr(T(y^{(rep)}, \theta) &gt; T(y, \theta) | y, \theta)\)</span>. If <span class="math inline">\(\theta\)</span> is unknown, then average over the posterior distribution of <span class="math inline">\(\theta\)</span>, <span class="math display">\[
\begin{aligned}[t]
p &amp;= \Pr(T(y^{(rep)}, \theta) &gt; T(y, \theta) | y) \\
&amp;= \int Pr(T(y^{(rep)}, \theta) &gt; T(y, \theta) | y, \theta) p(\theta | y) d\,\theta ,
\end{aligned}
\]</span> which is easily estimated from the MCMC samples as, <span class="math display">\[
p = \frac{1}{n_{sims}}\sum_{s = 1}^{n_{sims}} 1( T(y^{rep(s)}, \theta(s)) &gt; T(y, \theta(s)))
\]</span></p>
</div>
<div id="test-quantities" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Test quantities</h3>
<p>The definition of a posterior p-value does not specify a particular test-statistic, <span class="math inline">\(T\)</span>, to use.</p>
<p>The best advice is that <span class="math inline">\(T\)</span> depends on the application.</p>
<ul>
<li><span class="citation">@BDA3 [p. 146]</span> Speed of light example uses the 90% interval (61st and 6th order statistics).</li>
<li><span class="citation">@BDA3 [p. 147]</span> binomial trial example uses the number of swicthes (0 to 1, or 1 to 0) in order to test independence.</li>
<li><p><span class="citation">@BDA3 [p. 148]</span> hierarchical model for adolesce smoking uses</p>
<ul>
<li>percent of adolescents in the sample who never smoked</li>
<li>percentage in the sample who smoked in all waves</li>
<li>precentage of “incident smoker”: adolescents who began the study and non-smokers and ended as smokers.</li>
</ul></li>
</ul>
</div>
<div id="p-values-vs.u-values" class="section level3">
<h3><span class="header-section-number">3.2.3</span> p-values vs. u-values</h3>
<p>A posterior predictive p-value is different than a classical p-value.</p>
<ul>
<li><p>Posterior predictive p-value</p>
<ul>
<li>distributed uniform if the <strong>model is true</strong></li>
</ul></li>
<li><p>Classical p-value</p>
<ul>
<li>distributed uniform if the <strong>null hypothesis</strong> (<span class="math inline">\(H_0\)</span>) is true</li>
</ul></li>
</ul>
<p>A <em>u-value</em> is any function of the data that has a <span class="math inline">\(U(0, 1)\)</span> sampling distribution <span class="citation">[@BDA3, p. 151]</span></p>
<ul>
<li>a u-value can be averaged over <span class="math inline">\(\theta\)</span>, but it is not Bayesian, and is not a probability distribution</li>
<li>posterior p-value: probability statement, conditional on model and data, about future observations</li>
</ul>
</div>
<div id="marginal-predictive-checks" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Marginal predictive checks</h3>
<p>Compare statistics for each observation.</p>
<p><em>Conditional Predictive Ordinate (CPO)</em>: The CPO (Gelfand 1996) is the leave-on-out cross-validation predictive density: <span class="math display">\[
p(y_i | y_{-i}) = \int p(y_i | \theta) p(\theta | y_{-i}) d\,\theta
\]</span> The pointwise predicted LOO probabilities can be calculated using PSIS-LOO or WAIC in the <strong>loo</strong> package.</p>
<!-- The sum of the logged CPOs can be an estimator of the log marginal likelihood and is called the log pseudo marginal likelihood. The ratio of PsMLs can be used as a surrogate for a Bayes Factor (pseudo Bayes Factor) (LaplaceDemon p. 20) -->
<p><strong>Predictive Concordance and Predictive QuantilesP</strong> Gelfand (1996) classifies any <span class="math inline">\(y_i\)</span> that is outside the central 95% predictive posterior of <span class="math inline">\(y^{rep}_i\)</span> is an outlier. Let the <em>predictive quantile</em> (<span class="math inline">\(PQ_i\)</span>) be <span class="math display">\[
PQ_i = p(y_i^{(rep)} &gt; y_i) .
\]</span> Then the <em>predictive concordance</em> be the proportion of <span class="math inline">\(y_i\)</span> that are not outliers. Gelfand (1996) argues that the predictive concordance should match 95% - in other words that the posterior predictive distribution should have the correct coverage. (Laplace Demon p. 20)</p>
</div>
<div id="outliers" class="section level3">
<h3><span class="header-section-number">3.2.5</span> Outliers</h3>
<p>Can be identified by the inverse-CPO.</p>
<ul>
<li>larger than 40 are possible outliers, and those higher than 70 are extreme values (Ntzoufras 2009, p. 376).</li>
<li>Congdon (2005) scales CPO by dividing each by its individual max and considers observations with scaled CPO under 0.01 as outliers.</li>
</ul>
</div>
<div id="grapical-posterior-predictive-checks" class="section level3">
<h3><span class="header-section-number">3.2.6</span> Grapical Posterior Predictive Checks</h3>
<blockquote>
<p>Visualization can surprise you, but it doesn’t scale well. Modeling scales well, but it can’t surprise you. – <a href="https://www.johndcook.com/blog/2013/02/07/visualization-modeling-and-surprises/">paraphrase of Hadley Hickham</a></p>
</blockquote>
<p>Instead of calculating posterior probabilities, plot simulated data and observed data and visually compare them. See <span class="citation">@BDA3 [p. 154]</span>.</p>
<ul>
<li>plot simulated data and real data <span class="citation">[@BDA3, p. 154]</span>. This is similar to ideas in <span class="citation">@WickhamCookHofmannEtAl2010a</span>.</li>
<li>plot summary statistics or inferences</li>
<li><p>residual plots</p>
<ul>
<li>Bayesian residuals have a distribution <span class="math inline">\(r_i^{(s)} = y_i - \E(y_i | \theta^{s})\)</span></li>
<li>Bayesian resdiual graph plots single realization of the residuals, or a summary of their posterior distributions</li>
<li>binned plots are best for discrete data <span class="citation">[@BDA3, p. 157]</span></li>
<li></li>
</ul></li>
</ul>
<!--
## Average Predictive Comparisons

From @GelmanHill [Ch 21.4]
Let $u$ be the input of interest, and $v$ be all other inputs, so that $x = (u, v)$.
$$
b_u(u^{(lo)}, u^{(hi)}, v, \theta) = \frac{E(y | u^{(hi)}, v, \theta) - E(y | u^{(lo)}, v, \theta)}{u^{(hi)} - u^{(lo)}}
$$
the the average predictive difference per unit change in $u$ is,
$$
B_{u}(u^{(lo)}, u^{(hi)}) = \frac{1}{n} \sum_{i = 1}^n b_u(u^{(lo)}, u^{(hi)}, v_i, \theta) .
$$
This can be adjusted to use observed (weighted) differences of $u$ for each point.
See the Gelman paper on it.
-->
</div>
</div>
<div id="sources" class="section level2">
<h2><span class="header-section-number">3.3</span> Sources</h2>
<ul>
<li>See <span class="citation">@GelmanShalizi2012a</span>, <span class="citation">@GelmanShalizi2012b</span>, <span class="citation">@Kruschke2013b</span></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="posterior-inference.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>


<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jrnold/bayesian_notes/edit/master/posterior-predictive.Rmd",
"text": "Edit"
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
