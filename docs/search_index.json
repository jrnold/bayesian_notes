[
["heteroskedasticity-and-robust-regression.html", "10 Heteroskedasticity and Robust Regression 10.1 Prerequisites 10.2 Linear Regression with Student t distributed errors 10.3 Heteroskedasticity 10.4 References", " 10 Heteroskedasticity and Robust Regression 10.1 Prerequisites VGAM is needed for the Laplace distribution. library(&quot;VGAM&quot;) 10.2 Linear Regression with Student t distributed errors Like OLS, Bayesian linear regression with normally distributed errors is sensitive to outliers. The normal distribution has narrow tail probabilities. This plots the normal, Double Exponential (Laplace), and Student-t (df = 4) distributions all with mean 0 and scale 1, and the surprise (\\(- log(p)\\)) at each point. Higher surprise is a lower log-likelihood. Both the Student-t and Double Exponential distributions have surprise values well below the normal in the ranges (-6, 6).1 This means that outliers impose less of a penalty on the log-posterior models using these distributions, and the regression line would need to move less to incorporate those observations since the error distribution will not consider them as unusual. z &lt;- seq(-6, 6, length.out = 100) bind_rows( tibble(z = z, p = dnorm(z, 0, 1), distr = &quot;Normal&quot;), tibble(z = z, p = dt(z, 4), distr = &quot;Student-t (df = 4)&quot;), tibble(z = z, p = VGAM::dlaplace(z, 0, 1), distr = &quot;Double Exponential&quot;)) %&gt;% mutate(`-log(p)` = -log(p)) %&gt;% ggplot(aes(x = z, y = `-log(p)`, colour = distr)) + geom_line() print(mod_t) #&gt; S4 class stanmodel &#39;rlm&#39; coded as follows: #&gt; data { #&gt; // number of observations #&gt; int n; #&gt; // response vector #&gt; vector[n] y; #&gt; // number of columns in the design matrix X #&gt; int k; #&gt; // design matrix X #&gt; matrix [n, k] X; #&gt; // beta prior #&gt; real b_loc; #&gt; real&lt;lower = 0.0&gt; b_scale; #&gt; // sigma prior #&gt; real sigma_scale; #&gt; } #&gt; parameters { #&gt; // regression coefficient vector #&gt; vector[k] b; #&gt; // scale of the regression errors #&gt; real&lt;lower = 0.0&gt; sigma; #&gt; real&lt;lower = 1.0&gt; nu; #&gt; } #&gt; transformed parameters { #&gt; // mu is the observation fitted/predicted value #&gt; // also called yhat #&gt; vector[n] mu; #&gt; mu = X * b; #&gt; } #&gt; model { #&gt; // priors #&gt; b ~ normal(b_loc, b_scale); #&gt; sigma ~ cauchy(0, sigma_scale); #&gt; nu ~ gamma(2, 0.1); #&gt; // likelihood #&gt; y ~ student_t(nu, mu, sigma); #&gt; } #&gt; generated quantities { #&gt; // simulate data from the posterior #&gt; vector[n] y_rep; #&gt; // log-likelihood values #&gt; vector[n] log_lik; #&gt; for (i in 1:n) { #&gt; y_rep[i] = student_t_rng(nu, mu[i], sigma); #&gt; log_lik[i] = student_t_lpdf(y[i] | nu, mu[i], sigma); #&gt; } #&gt; #&gt; } unionization &lt;- read_tsv(&quot;data/western1995/unionization.tsv&quot;, col_types = cols( country = col_character(), union_density = col_double(), left_government = col_double(), labor_force_size = col_number(), econ_conc = col_double() )) mod_data &lt;- preprocess_lm(union_density ~ left_government + log(labor_force_size) + econ_conc, data = unionization) mod_data &lt;- within(mod_data, { b_loc &lt;- 0 b_scale &lt;- 100 sigma_scale &lt;- sd(y) }) The max_treedepth parameter needed to be increased because in some runs it was hitting the maximum tree depth. This is likely due to the wide tails of the Student t distribution. mod_t_fit &lt;- sampling(mod_t, data = mod_data, control = list(max_treedepth = 11)) #&gt; #&gt; SAMPLING FOR MODEL &#39;rlm&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 3.5e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.811644 seconds (Warm-up) #&gt; 0.646364 seconds (Sampling) #&gt; 1.45801 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 1 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;rlm&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.829618 seconds (Warm-up) #&gt; 0.838252 seconds (Sampling) #&gt; 1.66787 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 2 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;rlm&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.4e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.755767 seconds (Warm-up) #&gt; 0.710474 seconds (Sampling) #&gt; 1.46624 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 3 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is 0, but must be &gt; 0! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. #&gt; #&gt; SAMPLING FOR MODEL &#39;rlm&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.6e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.897055 seconds (Warm-up) #&gt; 0.758835 seconds (Sampling) #&gt; 1.65589 seconds (Total) #&gt; The following numerical problems occurred the indicated number of times on chain 4 #&gt; count #&gt; Exception thrown at line 35: student_t_lpdf: Scale parameter is inf, but must be finite! 1 #&gt; When a numerical problem occurs, the Hamiltonian proposal gets rejected. #&gt; See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected #&gt; If the number in the &#39;count&#39; column is small, there is no need to ask about this message on stan-users. summary(mod_t_fit, pars = c(&quot;nu&quot;, &quot;sigma&quot;, &quot;b&quot;))$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff #&gt; nu 21.699 0.24416 14.3626 3.599 11.279 18.431 28.918 57.369 3460 #&gt; sigma 10.441 0.04339 2.0681 7.059 9.015 10.166 11.610 15.230 2272 #&gt; b[1] 66.279 1.47339 53.0047 -43.640 32.578 67.960 102.590 166.056 1294 #&gt; b[2] 0.274 0.00149 0.0806 0.114 0.223 0.275 0.326 0.432 2914 #&gt; b[3] -4.494 0.09324 3.4316 -11.013 -6.858 -4.598 -2.252 2.676 1354 #&gt; b[4] 10.789 0.50310 18.3043 -23.366 -2.307 10.319 22.500 48.544 1324 #&gt; Rhat #&gt; nu 1.000 #&gt; sigma 1.001 #&gt; b[1] 1.002 #&gt; b[2] 0.999 #&gt; b[3] 1.002 #&gt; b[4] 1.002 Compare those results when using a model with mod_normal mod_normal_fit &lt;- sampling(mod_normal, data = mod_data) #&gt; #&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 1). #&gt; #&gt; Gradient evaluation took 2.7e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.483015 seconds (Warm-up) #&gt; 0.435345 seconds (Sampling) #&gt; 0.91836 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 2). #&gt; #&gt; Gradient evaluation took 9e-06 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.458847 seconds (Warm-up) #&gt; 0.349973 seconds (Sampling) #&gt; 0.80882 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 3). #&gt; #&gt; Gradient evaluation took 1.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.465702 seconds (Warm-up) #&gt; 0.335748 seconds (Sampling) #&gt; 0.80145 seconds (Total) #&gt; #&gt; #&gt; SAMPLING FOR MODEL &#39;lm&#39; NOW (CHAIN 4). #&gt; #&gt; Gradient evaluation took 1.1e-05 seconds #&gt; 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds. #&gt; Adjust your expectations accordingly! #&gt; #&gt; #&gt; Iteration: 1 / 2000 [ 0%] (Warmup) #&gt; Iteration: 200 / 2000 [ 10%] (Warmup) #&gt; Iteration: 400 / 2000 [ 20%] (Warmup) #&gt; Iteration: 600 / 2000 [ 30%] (Warmup) #&gt; Iteration: 800 / 2000 [ 40%] (Warmup) #&gt; Iteration: 1000 / 2000 [ 50%] (Warmup) #&gt; Iteration: 1001 / 2000 [ 50%] (Sampling) #&gt; Iteration: 1200 / 2000 [ 60%] (Sampling) #&gt; Iteration: 1400 / 2000 [ 70%] (Sampling) #&gt; Iteration: 1600 / 2000 [ 80%] (Sampling) #&gt; Iteration: 1800 / 2000 [ 90%] (Sampling) #&gt; Iteration: 2000 / 2000 [100%] (Sampling) #&gt; #&gt; Elapsed Time: 0.54117 seconds (Warm-up) #&gt; 0.407283 seconds (Sampling) #&gt; 0.948453 seconds (Total) summary(mod_normal_fit, pars = c(&quot;b&quot;, &quot;sigma&quot;))$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff #&gt; b[1] 72.930 1.5774 52.9423 -32.488 38.804 72.844 108.590 175.722 1127 #&gt; b[2] 0.269 0.0019 0.0813 0.105 0.217 0.268 0.321 0.431 1835 #&gt; b[3] -4.859 0.1048 3.5445 -11.707 -7.235 -4.898 -2.621 2.184 1143 #&gt; b[4] 8.368 0.5112 17.6960 -25.508 -3.307 8.001 20.290 43.378 1198 #&gt; sigma 11.070 0.0621 2.1395 7.879 9.582 10.739 12.195 16.245 1188 #&gt; Rhat #&gt; b[1] 1 #&gt; b[2] 1 #&gt; b[3] 1 #&gt; b[4] 1 #&gt; sigma 1 Alternatively, the Double Exponential (Laplace) distribution can be used for the errors. This is the equivalent to least quantile regression, where the regression line is the median (50% quantile) mod_dbl_exp summary(mod_dbl_exp_fit, par = c(&quot;b&quot;, &quot;sigma&quot;))$summary #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff #&gt; b[1] 38.693 1.90247 51.2313 -60.417 5.004 37.97 71.661 140.290 725 #&gt; b[2] 0.298 0.00225 0.0837 0.131 0.245 0.30 0.352 0.458 1387 #&gt; b[3] -2.971 0.11773 3.2419 -9.408 -5.073 -3.01 -0.842 3.242 758 #&gt; b[4] 20.981 0.67250 18.2690 -15.589 9.157 21.53 33.057 56.774 738 #&gt; sigma 9.050 0.06676 2.2260 5.585 7.533 8.75 10.209 14.423 1112 #&gt; Rhat #&gt; b[1] 1 #&gt; b[2] 1 #&gt; b[3] 1 #&gt; b[4] 1 #&gt; sigma 1 10.3 Heteroskedasticity In applied regression, heteroskedasticity consistent (HC) or robust standard errors are often used. However, there is straightforwardly direct translation of HC standard error to regression model this in a Bayesian setting. The sandwich method of estimating HC errors uses the same point estimates for the regression coefficients as OLS, but estimates the standard errors of those coefficients in a second stage from the OLS residuals. Disregarding differences in frequentist vs. Bayesian inference, it is clear that a direct translation of that method could not be fully Bayesian since the coefficients and errors are not estimated jointly. In a linear normal regression model with heteroskedasticity, each observation has its own scale parameter, \\(\\sigma_i\\), \\[ \\begin{aligned}[t] y_i &amp;\\sim \\dnorm(X \\beta, \\sigma_i) . \\end{aligned} \\] It should be clear that without proper priors this model is not identified, meaning that the posterior distribution is improper. To estimate this model we have to apply some model to the scale terms, \\(\\sigma_i\\). In fact, you can think of homoskedasticity as the simplest such model; assuming that all \\(\\sigma_i = \\sigma\\). A more general model of \\(\\sigma_i\\) should encode any information the analyst has about the scale terms. This can be a distribution or functions of covariates for how we think observations may have different values. 10.3.1 Covariates A simple model of heteroskedasticity is if the observations can be split into groups. Suppose the observations are partitioned into \\(k = 1, \\dots, K\\) groups, and \\(k[i]\\) is the group of observation \\(i\\), \\[ \\sigma_i = \\sigma_{k[i]} \\] Another choice would be to model the scale term with a regression model, for example, \\[ \\log(\\sigma_i) \\sim \\dnorm(X \\gamma, \\tau) \\] 10.3.2 Student-t It turns out that the Student-t distribution of error terms from the Robust Regression chapter can also be derived as a model of heteroskedasticity. A reparameterization that will be used quite often is to rewrite a normal distributions with unequal scale parameters as a continuous mixture of a common global scale parameter (\\(\\sigma\\)), and observation specific local scale parameters, \\(\\lambda_i\\),[^globalmixture] \\[ y_i \\sim \\dnorm(X\\beta, \\lambda_i \\sigma) . \\] If the local scale parameters are distributed as, \\[ \\lambda^2 \\sim \\dinvgamma(\\nu / 2, \\nu / 2) \\] then the above is equivalent to a regression with errors distributed Student-t errors with \\(\\nu\\) degrees of freedom, \\[ y_i \\sim \\dt{\\nu}(X \\beta, \\sigma) . \\] [^globalmixture] See this for a visualization of a Student-t distribution a mixture of Normal distributions, and this for a derivation of the Student t distribution as a mixture of normal distributions. This scale mixture of normal representation will also be used with shrinkage priors on the regression coefficients. Example: Simulate Student-t distribution with \\(\\nu\\) degrees of freedom as a scale mixture of normal. For *s in 1:S$, Simulate \\(z_s \\sim \\dgamma(\\nu / 2, \\nu / 2)\\) \\(x_s = 1 / \\sqrt{z_s}2\\) is draw from \\(\\dt{\\nu}(0, 1)\\). When using R, ensure that you are using the correct parameterization of the gamma distribution. Left to reader 10.4 References 10.4.1 Robust regression See Gelman and Hill (2007 sec 6.6), Gelman et al. (2013 ch 17) Stan Development Team (2016 Sec 8.4) for the Stan example using a Student-t distribution 10.4.2 Heteroskedasticity Gelman et al. (2013 Sec. 14.7) for models with unequal variances and correlations. Stan Development Team (2016) reparameterizes the Student t distribution as a mixture of gamma distributions in Stan. The Double Exponential distribution still has a thinner tail than the Student-t at higher values.â†© "]
]
